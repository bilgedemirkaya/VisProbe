{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VisProbe Quick Start Guide\n",
    "\n",
    "**Find robustness failures in your vision models in 5 minutes.**\n",
    "\n",
    "This notebook demonstrates how to use VisProbe to test your model's robustness against natural perturbations.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. ‚úÖ How to test a model with just 3 lines of code\n",
    "2. ‚úÖ How to interpret robustness scores and failures\n",
    "3. ‚úÖ How to export failures for model improvement\n",
    "4. ‚úÖ How to choose the right preset for your use case\n",
    "\n",
    "**Time to complete:** 5-10 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "First, make sure VisProbe is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you haven't installed VisProbe yet\n",
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import and Setup\n",
    "\n",
    "Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# Import VisProbe\n",
    "from visprobe import quick_check\n",
    "\n",
    "print(\"‚úì Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load a Model\n",
    "\n",
    "For this example, we'll use a pretrained ResNet-18. You can replace this with your own model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained model\n",
    "model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úì Model loaded: ResNet-18\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Test Data\n",
    "\n",
    "Load some test images. We'll use CIFAR-10 for this demo.\n",
    "\n",
    "> **Note:** In production, use images that match your model's training distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare transforms\n",
    "transform = T.Compose([\n",
    "    T.Resize(224),  # ResNet expects 224x224\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Take a subset for faster testing\n",
    "num_samples = 50  # Increase this for more thorough testing\n",
    "test_data = [dataset[i] for i in range(num_samples)]\n",
    "\n",
    "print(f\"‚úì Loaded {num_samples} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Robustness Test üöÄ\n",
    "\n",
    "Now for the magic! Test your model with just one function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run robustness test\n",
    "report = quick_check(\n",
    "    model=model,\n",
    "    data=test_data,\n",
    "    preset=\"lighting\",  # Test lighting variations\n",
    "    budget=500,         # Number of model queries (increase for more precision)\n",
    "    device=\"auto\"       # Auto-detect GPU/CPU\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. View Results üìä\n",
    "\n",
    "The `report.show()` method displays a rich HTML summary in Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results (will show rich HTML in Jupyter)\n",
    "report.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Results Programmatically\n",
    "\n",
    "Access the results as Python objects for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall robustness score (0-1, higher is better)\n",
    "print(f\"Robustness Score: {report.score:.1%}\")\n",
    "\n",
    "# Number of failures found\n",
    "print(f\"Total Failures: {len(report.failures)}\")\n",
    "\n",
    "# Test metadata\n",
    "print(f\"Runtime: {report.summary['runtime_sec']:.1f}s\")\n",
    "print(f\"Model Queries: {report.summary['model_queries']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Score\n",
    "\n",
    "The robustness score tells you how well your model handles perturbations:\n",
    "\n",
    "- **> 80%**: ‚úÖ Excellent - Model is highly robust\n",
    "- **60-80%**: ‚úÖ Good - Reasonable robustness with some weaknesses\n",
    "- **40-60%**: ‚ö†Ô∏è Moderate - Significant robustness issues\n",
    "- **< 40%**: ‚ùå Poor - Model is very fragile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the score\n",
    "score = report.score\n",
    "\n",
    "if score > 0.8:\n",
    "    print(\"‚úÖ EXCELLENT - Your model is highly robust!\")\n",
    "elif score > 0.6:\n",
    "    print(\"‚úÖ GOOD - Reasonable robustness with room for improvement.\")\n",
    "elif score > 0.4:\n",
    "    print(\"‚ö†Ô∏è MODERATE - Your model has significant robustness issues.\")\n",
    "else:\n",
    "    print(\"‚ùå POOR - Your model is very fragile to perturbations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inspect Failures\n",
    "\n",
    "Look at specific failure cases to understand what went wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 5 failures\n",
    "if report.failures:\n",
    "    print(f\"Found {len(report.failures)} failure cases:\\n\")\n",
    "    \n",
    "    for i, failure in enumerate(report.failures[:5], 1):\n",
    "        print(f\"{i}. Sample {failure['index']}:\")\n",
    "        print(f\"   Original prediction: {failure['original_pred']}\")\n",
    "        print(f\"   Perturbed prediction: {failure['perturbed_pred']}\")\n",
    "        print(f\"   Perturbation level: {failure['level']:.3f}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No failures found! Your model is robust to this preset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Failures for Retraining\n",
    "\n",
    "Export the worst failures to use as hard examples in your training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if report.failures:\n",
    "    # Export top 10 failures\n",
    "    export_path = report.export_failures(n=10)\n",
    "    print(f\"‚úÖ Exported 10 failure cases to:\")\n",
    "    print(f\"   {export_path}\")\n",
    "    print(f\"\\nüí° Use these failures to:\")\n",
    "    print(f\"   1. Understand your model's weak points\")\n",
    "    print(f\"   2. Add similar examples to your training set\")\n",
    "    print(f\"   3. Increase data augmentation in problem areas\")\n",
    "else:\n",
    "    print(\"No failures to export!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Try Different Presets\n",
    "\n",
    "VisProbe includes 4 presets for different use cases. Let's try the \"standard\" preset which includes compositional perturbations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available presets\n",
    "from visprobe import presets\n",
    "\n",
    "print(\"Available presets:\\n\")\n",
    "for name, description in presets.list_presets():\n",
    "    print(f\"  ‚Ä¢ {name:12s}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with \"standard\" preset (includes compositional perturbations!)\n",
    "report_standard = quick_check(\n",
    "    model=model,\n",
    "    data=test_data[:20],  # Use fewer samples for faster demo\n",
    "    preset=\"standard\",\n",
    "    budget=300,\n",
    "    device=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"\\nStandard Preset Results:\")\n",
    "print(f\"  Score: {report_standard.score:.1%}\")\n",
    "print(f\"  Failures: {len(report_standard.failures)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preset Comparison\n",
    "\n",
    "Compare results across presets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two presets\n",
    "print(\"\\nüìä Preset Comparison:\\n\")\n",
    "print(f\"  Lighting:  {report.score:.1%} ({len(report.failures)} failures)\")\n",
    "print(f\"  Standard:  {report_standard.score:.1%} ({len(report_standard.failures)} failures)\")\n",
    "\n",
    "# Which is weaker?\n",
    "if report.score < report_standard.score:\n",
    "    print(f\"\\n‚ö†Ô∏è  Your model is weaker on lighting perturbations\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Your model is weaker on standard perturbations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Complete Summary\n",
    "\n",
    "Get a complete summary dict for programmatic use (e.g., CI/CD checks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary dictionary\n",
    "summary = report.summary\n",
    "\n",
    "print(\"Full Summary:\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've learned how to use VisProbe to test your model's robustness!\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "‚úÖ Tested a model with `quick_check()` in 3 lines  \n",
    "‚úÖ Viewed results with `report.show()`  \n",
    "‚úÖ Analyzed failures programmatically  \n",
    "‚úÖ Exported failures for retraining  \n",
    "‚úÖ Compared different presets  \n",
    "\n",
    "---\n",
    "\n",
    "## üìö Next Steps\n",
    "\n",
    "### For Your Own Model\n",
    "\n",
    "1. **Replace the model:**\n",
    "   ```python\n",
    "   model = YourModel()\n",
    "   model.load_state_dict(torch.load('your_weights.pth'))\n",
    "   ```\n",
    "\n",
    "2. **Use your test data:**\n",
    "   ```python\n",
    "   test_data = your_dataset  # Can be DataLoader, list, or tensors\n",
    "   ```\n",
    "\n",
    "3. **Set correct normalization:**\n",
    "   ```python\n",
    "   report = quick_check(\n",
    "       model, data, preset=\"standard\",\n",
    "       mean=(0.485, 0.456, 0.406),  # Your training mean\n",
    "       std=(0.229, 0.224, 0.225)     # Your training std\n",
    "   )\n",
    "   ```\n",
    "\n",
    "### Advanced Usage\n",
    "\n",
    "- See `examples/custom_model_example.py` for a complete template\n",
    "- See `examples/preset_comparison.py` to compare all presets\n",
    "- See `README.md` for advanced configuration options\n",
    "\n",
    "### Production Deployment\n",
    "\n",
    "Use VisProbe in your CI/CD pipeline:\n",
    "\n",
    "```python\n",
    "report = quick_check(model, test_data, preset=\"standard\")\n",
    "assert report.score > 0.7, f\"Model robustness too low: {report.score:.1%}\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù Contributing\n",
    "\n",
    "Found this helpful? Give us a star on GitHub! ‚≠ê\n",
    "\n",
    "Have questions or issues? Open an issue on GitHub.\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Resources\n",
    "\n",
    "- **Main README**: [../README.md](../README.md)\n",
    "- **Examples**: [../examples/](../examples/)\n",
    "- **API Reference**: [../COMPREHENSIVE_API_REFERENCE.md](../COMPREHENSIVE_API_REFERENCE.md)\n",
    "- **Troubleshooting**: [../TROUBLESHOOTING.md](../TROUBLESHOOTING.md)\n",
    "\n",
    "Happy testing! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
